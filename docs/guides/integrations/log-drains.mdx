---
title: "Log Drains"
description: "Forward Formal Connector logs to your SIEM, data lake, or logging platform"
icon: "pipe"
---

## Overview

Formal can forward all Connector activity logs to your SIEM, data lake, or logging platform. This enables centralized log management, long-term retention, compliance reporting, and integration with your existing security tools.

## Supported Platforms

<img src="/assets/images/supported_logs.png" alt="Supported log destinations" />

- **AWS S3**
- **Datadog**
- **Splunk**
- **Elastic**
- **Sumo Logic**
- **Any S3-compatible storage**

<Note>
  By default, Formal forwards **logs from all Connectors** to your configured
  destination.
</Note>

## Log Contents

Each log entry includes:

| Field              | Description                                      |
| ------------------ | ------------------------------------------------ |
| **Timestamp**      | When the query/command was executed              |
| **User**           | Formal user who made the request                 |
| **End-User**       | Actual end-user (for BI tool queries)            |
| **Resource**       | Target database, API, or infrastructure          |
| **Connector**      | Which Connector processed the request            |
| **Query/Command**  | Full query text or command                       |
| **Response**       | Response data (can be configured to exclude PII) |
| **Policy Actions** | Any policy enforcement actions taken             |
| **Session ID**     | Associated session identifier                    |
| **Client IP**      | Source IP address                                |
| **Duration**       | Query execution time                             |

## Setup

### AWS S3

<Warning>
  AWS S3 log integration requires an [AWS Cloud
  Integration](/docs/guides/integrations/cloud-accounts) with S3 access enabled.
</Warning>

<Steps>
  <Step title="Set Up AWS Integration">
    First, configure an [AWS Cloud
    Integration](/docs/guides/integrations/cloud-accounts) with: -
    **AllowS3Access**: `true` - **S3BucketARN**: Your S3 bucket ARN
  </Step>
  <Step title="Navigate to Log Integrations">
    Go to [Log Integrations](https://app.joinformal.com/log-integrations)
  </Step>
  <Step title="Create Integration">Click **Create Log Integration**</Step>
  <Step title="Select AWS S3">Choose AWS S3 as your provider</Step>
  <Step title="Configure">
    - **S3 Bucket Name**: Your bucket name - **Cloud Integration**: Select your
    AWS Cloud Integration
  </Step>
</Steps>

#### Terraform

```hcl
# First, create S3 bucket
resource "aws_s3_bucket" "formal_logs" {
  bucket = "formal-connector-logs"
}

# Create AWS Cloud Integration
resource "formal_integration_cloud" "aws" {
  name         = "aws-integration"
  cloud_region = "us-east-1"

  aws {
    template_version    = "1.2.0"
    allow_s3_access     = true
    s3_bucket_arn       = "${aws_s3_bucket.formal_logs.arn}/*"
  }
}

# Deploy CloudFormation stack (see AWS Integration guide)
resource "aws_cloudformation_stack" "formal" {
  # ... (see AWS Integration docs)
}

# Create log integration
resource "formal_integration_log" "s3" {
  name = "s3-log-drain"

  s3 {
    s3_bucket_name       = aws_s3_bucket.formal_logs.bucket
    cloud_integration_id = formal_integration_cloud.aws.id
  }
}
```

### Datadog

<Steps>
  <Step title="Get Datadog Credentials">
    From your Datadog account, retrieve: - Account ID - API Key - Site (e.g.,
    `datadoghq.com`, `datadoghq.eu`)
  </Step>
  <Step title="Navigate to Log Integrations">
    Go to [Log Integrations](https://app.joinformal.com/log-integrations)
  </Step>
  <Step title="Create Integration">Click **Create Log Integration**</Step>
  <Step title="Select Datadog">Choose Datadog as your provider</Step>
  <Step title="Enter Credentials">
    - **Account ID**: Your Datadog account ID - **API Key**: Your Datadog API
    key - **Site**: Your Datadog site
  </Step>
</Steps>

#### Terraform

```hcl
resource "formal_integration_log" "datadog" {
  name = "datadog-log-drain"

  datadog {
    account_id = var.datadog_account_id
    api_key    = var.datadog_api_key
    site       = "datadoghq.com"  # or datadoghq.eu, etc.
  }
}
```

### Splunk

<Steps>
  <Step title="Create Splunk HEC Token">
    In Splunk, create a new HTTP Event Collector (HEC) token
  </Step>
  <Step title="Navigate to Log Integrations">
    Go to [Log Integrations](https://app.joinformal.com/log-integrations)
  </Step>
  <Step title="Create Integration">Click **Create Log Integration**</Step>
  <Step title="Select Splunk">Choose Splunk as your provider</Step>
  <Step title="Enter Configuration">
    - **Access Token**: Your HEC token - **Host**: Your Splunk instance hostname
    - **Port**: HEC port (usually 8088)
  </Step>
</Steps>

#### Terraform

```hcl
resource "formal_integration_log" "splunk" {
  name = "splunk-log-drain"

  splunk {
    access_token = var.splunk_hec_token
    host         = "splunk.example.com"
    port         = 8088
  }
}
```

## Log Format

Logs are forwarded in JSON format:

```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "session_id": "sess_abc123",
  "user": {
    "username": "idp:formal:human:alice@example.com",
    "email": "alice@example.com",
    "type": "human"
  },
  "end_user": {
    "username": "idp:formal:human:alice@example.com",
    "email": "alice@example.com",
    "type": "human"
  },
  "resource": {
    "id": "res_xyz789",
    "name": "production-postgres",
    "technology": "postgres",
    "hostname": "prod-db.us-east-1.rds.amazonaws.com"
  },
  "connector": {
    "id": "conn_def456",
    "name": "production-connector"
  },
  "query": {
    "query": "SELECT id, name, email FROM users WHERE id = 123",
    "statement_type": "SELECT",
    "duration_ms": 45
  },
  "policy_actions": [
    {
      "policy_id": "pol_ghi789",
      "policy_name": "mask-pii",
      "action": "mask",
      "reason": "PII protection"
    }
  ],
  "client": {
    "ip_address": "203.0.113.10",
    "application": "psql"
  }
}
```

## Use Cases

### Compliance and Auditing

Forward logs to long-term storage for compliance requirements:

```hcl
# Archive to S3 for 7 years (SOC 2, HIPAA, etc.)
resource "formal_integration_log" "compliance_archive" {
  name = "compliance-s3-archive"

  s3 {
    s3_bucket_name       = aws_s3_bucket.compliance_logs.bucket
    cloud_integration_id = formal_integration_cloud.aws.id
  }
}

# Configure S3 lifecycle policy
resource "aws_s3_bucket_lifecycle_configuration" "compliance" {
  bucket = aws_s3_bucket.compliance_logs.id

  rule {
    id     = "archive-old-logs"
    status = "Enabled"

    transition {
      days          = 90
      storage_class = "GLACIER"
    }

    expiration {
      days = 2555  # 7 years
    }
  }
}
```

### Real-Time Security Monitoring

Send logs to your SIEM for real-time threat detection:

```hcl
# Send to Datadog for real-time monitoring
resource "formal_integration_log" "security_monitoring" {
  name = "datadog-security"

  datadog {
    account_id = var.datadog_account_id
    api_key    = var.datadog_api_key
    site       = "datadoghq.com"
  }
}
```

Create alerts in Datadog for:

- Failed authentication attempts
- Policy violations
- Unusual query patterns
- Off-hours access

### Data Lake Integration

Forward logs to your data lake for analytics:

```hcl
# Send to S3 data lake
resource "formal_integration_log" "data_lake" {
  name = "data-lake-integration"

  s3 {
    s3_bucket_name       = "my-data-lake-formal-logs"
    cloud_integration_id = formal_integration_cloud.aws.id
  }
}
```

Then use Athena, Redshift Spectrum, or Databricks to analyze:

- User access patterns
- Query performance
- Policy effectiveness
- Resource utilization

### Multi-Destination Forwarding

Send logs to multiple destinations:

```hcl
# Real-time monitoring
resource "formal_integration_log" "splunk_realtime" {
  name = "splunk-realtime"
  splunk {
    access_token = var.splunk_hec_token
    host         = "splunk.example.com"
    port         = 8088
  }
}

# Long-term archive
resource "formal_integration_log" "s3_archive" {
  name = "s3-archive"
  s3 {
    s3_bucket_name       = "formal-logs-archive"
    cloud_integration_id = formal_integration_cloud.aws.id
  }
}

# Security analytics
resource "formal_integration_log" "datadog_security" {
  name = "datadog-security"
  datadog {
    account_id = var.datadog_account_id
    api_key    = var.datadog_api_key
    site       = "datadoghq.com"
  }
}
```

## Filtering Logs (Future)

Currently, all Connector logs are forwarded. Filtering capabilities (by Connector, resource, user, etc.) are on the roadmap.

## Privacy and Security

### PII in Logs

Logs may contain query results that include PII. Consider:

1. **Encrypt at rest**: Use S3 encryption, Datadog encryption, etc.
2. **Access controls**: Restrict who can access log data
3. **Retention policies**: Automatically delete old logs
4. **Redaction**: Configure policies to mask data before it's logged (coming soon)

### Secure Transmission

All log forwarding uses encrypted connections (HTTPS, TLS).

## Monitoring Log Delivery

Check log integration status:

1. Navigate to [Log Integrations](https://app.joinformal.com/log-integrations)
2. View integration status and last delivery time
3. Check for delivery errors or warnings

If logs aren't being delivered:

- Verify credentials are correct
- Check network connectivity from Formal to your platform
- Review integration status for error messages

## Best Practices

<AccordionGroup>
  <Accordion title="Multiple Destinations" icon="route">
    Send logs to both a SIEM (real-time) and S3 (archive) for comprehensive
    coverage.
  </Accordion>

<Accordion title="Retention Policies" icon="clock">
  Configure appropriate retention based on compliance requirements (SOC 2: 1
  year, HIPAA: 6 years, etc.).
</Accordion>

<Accordion title="Access Controls" icon="lock">
  Restrict access to log dataâ€”it contains sensitive queries and potentially PII.
</Accordion>

<Accordion title="Alerting" icon="bell">
  Set up alerts in your SIEM for policy violations, failed authentications, and
  anomalous behavior.
</Accordion>

  <Accordion title="Regular Reviews" icon="magnifying-glass">
    Periodically review logs to identify access patterns, policy gaps, and
    security incidents.
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="AWS Integration"
    icon="aws"
    href="/docs/guides/integrations/cloud-accounts"
  >
    Set up AWS Cloud Integration for S3 logs
  </Card>
  <Card
    title="View Logs"
    icon="file-lines"
    href="/docs/guides/observability/logs"
  >
    Monitor logs in the Formal console
  </Card>
  <Card
    title="Write Policies"
    icon="shield-check"
    href="/docs/guides/policies/policies"
  >
    Create policies that generate audit logs
  </Card>
  <Card
    title="Compliance"
    icon="file-certificate"
    href="/docs/guides/how-to/log-audits"
  >
    Use logs for compliance reporting
  </Card>
</CardGroup>
